      For this semesters programming assignment, we were tasked to create a chat bot that would take a user’s questions about a certain South Carolina state representative and return the pertinent information from the South Carolina State Legislature website. The chat bot was to be coded in the student’s choice of language; I chose to complete it in Python. For this programming project I implemented various Python libraries I chose to constrain my development to a console-based program and to solely answer questions that were answered by the website’s information. This information was limited to the representative for District 35. This program takes in a user query and then matches elements of that query to known facts about the representative in question. This then returns these facts and asks again what the user want to know. This is repeated until the quit case is meet, that being "q", "quit", or "exit". A history of the information received in this session as well as time elapsed are in the "stats.txt" file. For this assignment I first started with gathering the information from the legislature website. This was done in a file that I called scrapper.py and inside that I had a function that could be called in later parts of this project. I used a package called BeautifulSoup to parse and lint the website html that was gathered from a package called requests. This was then written into a text file that would be the database for the future retrieval of information. After the information was gathered, I had to process said information to return the right information based on the users’ requests. This was done by the function called process, which held lists of key words that correspond to the information type the user wants information about. I then took the users question and checked it for keywords that were contained in the aforementioned lists.  If a match was found then the user would receive the list of data that was linked to that keyword, and that same information would be written to the history in the stats file. All these processes were called to be run in the main method in the mai.py file. This method is only run however if the code was run directly and not imported adding the security and checking the run cases. The reusability of my loading animation as well as the basic structure of the web parser added to the simplicity of adapting them to other use cases. With the web parser all one must do to adapt is to change the URL and put in what information they are looking for on the website of choice, this also means that is I but in another representatives URL that the information for that representative will be returned without having to change anything.  The biggest challenge I face was parsing though the different HTML tags that the information I wanted to scrape was surrounded in, my work around was to instead remove everything I didn’t want. My loading animation was adapted from a public version of the same animation. In the future development of this chatbot program I would like to implement multiple districts so that any representative’s information can be used to answer a user’s questions. Another thing I would like to implement is the use of a language engine so that I can detect and respond to users who are interacting with the system in another language than English. 